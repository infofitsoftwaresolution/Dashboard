================================================================================
    COMPLETE SETUP GUIDE: S3 Trigger → Lambda → Extract Data → PostgreSQL
================================================================================

YOUR REQUIREMENT:
✅ When new parquet file added to S3
✅ Lambda automatically triggers
✅ Extract ALL data from parquet file
✅ Save ALL data to PostgreSQL database

================================================================================
                         CURRENT STATUS
================================================================================

✅ Lambda Function Code: READY (lambda_function.py)
   - Triggers on S3 parquet file upload
   - Reads parquet file from S3
   - Extracts all data
   - Inserts into PostgreSQL

✅ S3 Trigger: Need to configure
✅ Lambda Layer: Need Linux-compatible version (Windows version won't work)

================================================================================
                         SOLUTION: 3 OPTIONS
================================================================================

OPTION 1: Use Docker to Build Layer (RECOMMENDED)
--------------------------------------------------
1. Make sure Docker Desktop is RUNNING
2. Run this command:
   cd D:\Dashboard\lambda\parquet_to_rds
   .\build_linux_layer.ps1

3. Wait 5-10 minutes for build to complete
4. Upload lambda-layer-linux.zip to S3
5. Create Lambda Layer from S3
6. Attach layer to function

OPTION 2: Use AWS SAM (AUTOMATIC)
----------------------------------
1. Make sure Docker Desktop is RUNNING
2. Run:
   cd D:\Dashboard\lambda\parquet_to_rds
   sam build --use-container
   sam deploy --guided

3. Enter your credentials when prompted
4. SAM will create everything automatically

OPTION 3: Manual Fix (QUICKEST)
---------------------------------
1. Go to Lambda Console → Your Function
2. Remove the current layer (has Windows dependencies)
3. Go to Lambda → Layers → Create layer
4. Use AWS Data Wrangler layer:
   ARN: arn:aws:lambda:ap-south-1:336392948345:layer:AWSSDKPandas-Python311:2
5. Create small layer for psycopg2 only (use EC2 Linux or Docker)

================================================================================
                         STEP-BY-STEP: OPTION 3 (QUICKEST)
================================================================================

STEP 1: Remove Current Layer
-----------------------------
1. Lambda Console → parquet-to-rds-processor
2. Scroll to "Layers" section
3. Click "X" to remove current layer

STEP 2: Add AWS Data Wrangler Layer (has pandas & pyarrow)
-----------------------------------------------------------
1. Click "Add a layer"
2. Select "Specify an ARN"
3. Enter: arn:aws:lambda:ap-south-1:336392948345:layer:AWSSDKPandas-Python311:2
4. Click "Add"

STEP 3: Create psycopg2 Layer (Small - ~5 MB)
----------------------------------------------
Option A: Use EC2 Linux Instance
1. Launch EC2 (Amazon Linux 2)
2. SSH and run:
   mkdir -p layer/python/lib/python3.11/site-packages
   pip3.11 install psycopg2-binary==2.9.9 -t layer/python/lib/python3.11/site-packages/
   cd layer
   zip -r ../psycopg2-layer.zip .
3. Download ZIP and upload to S3
4. Create Lambda Layer from S3

Option B: Use Docker (if running)
1. Run:
   docker run --rm --entrypoint="" -v "${PWD}:/var/task" -w /var/task public.ecr.aws/lambda/python:3.11 /var/lang/bin/pip install psycopg2-binary==2.9.9 -t layer/python/lib/python3.11/site-packages/
   Compress-Archive -Path layer\* -DestinationPath psycopg2-layer.zip -Force
2. Upload to S3 and create layer

STEP 4: Attach psycopg2 Layer
-----------------------------
1. Lambda → Layers → Add layer
2. Select your psycopg2 layer
3. Click "Add"

STEP 5: Verify Configuration
---------------------------
✅ Function code uploaded
✅ AWS Data Wrangler layer attached
✅ psycopg2 layer attached
✅ Environment variables set:
   - DB_HOST = database-1.c3ea24kmsrmf.ap-south-1.rds.amazonaws.com
   - DB_PORT = 5432
   - DB_NAME = audit_trail_db
   - DB_USER = postgres
   - DB_PASSWORD = Dashboard6287
   - TABLE_NAME = audit_trail_data
✅ S3 trigger configured:
   - Bucket: your-bucket-name
   - Prefix: audit-trail-data/raw/
   - Suffix: .parquet

STEP 6: Test
-----------
1. Upload test.parquet to S3:
   aws s3 cp test.parquet s3://your-bucket/audit-trail-data/raw/test.parquet

2. Wait 1-2 minutes

3. Check Lambda:
   - Monitor tab → Should show 1 invocation
   - CloudWatch Logs → Should show "Successfully inserted X records"

4. Check PostgreSQL:
   psql -h database-1.c3ea24kmsrmf.ap-south-1.rds.amazonaws.com -U postgres -d audit_trail_db
   SELECT COUNT(*) FROM audit_trail_data;

================================================================================
                         HOW IT WORKS
================================================================================

1. You upload parquet file to S3:
   s3://your-bucket/audit-trail-data/raw/myfile.parquet

2. S3 automatically triggers Lambda function

3. Lambda function:
   - Reads parquet file from S3
   - Extracts ALL rows and columns
   - Connects to PostgreSQL
   - Inserts ALL data into audit_trail_data table

4. Data is now in PostgreSQL database!

================================================================================
                         TROUBLESHOOTING
================================================================================

Error: "module 'os' has no attribute 'add_dll_directory'"
→ Solution: Remove Windows layer, use Linux-compatible layer

Error: "No module named 'pandas'"
→ Solution: Attach AWS Data Wrangler layer

Error: "No module named 'psycopg2'"
→ Solution: Create and attach psycopg2 layer

Lambda not triggered:
→ Check S3 trigger is enabled
→ Verify file prefix/suffix matches
→ Check CloudWatch Logs for trigger events

No data in database:
→ Check CloudWatch Logs for errors
→ Verify database credentials
→ Check table exists: SELECT * FROM audit_trail_data LIMIT 1;

================================================================================
                         QUICK COMMANDS
================================================================================

# Upload test file
aws s3 cp test.parquet s3://your-bucket/audit-trail-data/raw/test.parquet

# Check Lambda logs
aws logs tail /aws/lambda/parquet-to-rds-processor --follow

# Check database
psql -h database-1.c3ea24kmsrmf.ap-south-1.rds.amazonaws.com -U postgres -d audit_trail_db -c "SELECT COUNT(*) FROM audit_trail_data;"

================================================================================



